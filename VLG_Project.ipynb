{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **Mario RL Project**\n",
    "This project aims to train a Mario agent that can play complete the first level of the game on its own using **Double Deep Q-networks** "
   ],
   "metadata": {
    "id": "Xww_YyXyXSR8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Installing the required dependencies**"
   ],
   "metadata": {
    "id": "21Pb8jCMXEzU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvNpbjMnEpZ8",
    "outputId": "d7d5e4de-c53b-4485-d0c6-6526eecdfaad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting nes-py==0.2.6\n",
      "  Downloading nes_py-0.2.6.tar.gz (75 kB)\n",
      "\u001B[K     |████████████████████████████████| 75 kB 5.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: gym>=0.10.5 in /usr/local/lib/python3.7/dist-packages (from nes-py==0.2.6) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from nes-py==0.2.6) (1.21.6)\n",
      "Collecting pygame>=1.9.3\n",
      "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
      "\u001B[K     |████████████████████████████████| 21.8 MB 90.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from nes-py==0.2.6) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.7/dist-packages (from nes-py==0.2.6) (4.64.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->nes-py==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->nes-py==0.2.6) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.3.2->nes-py==0.2.6) (0.16.0)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for nes-py: filename=nes_py-0.2.6-cp37-cp37m-linux_x86_64.whl size=168611 sha256=aede218f19d03b770761839b67694aad70a1fed4b030738432b6b43a9ef31ec2\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/87/a9/d777bc0614683325afc2501fe16a01ae29a9bf6c5650cffbad\n",
      "Successfully built nes-py\n",
      "Installing collected packages: pygame, nes-py\n",
      "Successfully installed nes-py-0.2.6 pygame-2.1.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym-super-mario-bros\n",
      "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
      "\u001B[K     |████████████████████████████████| 199 kB 33.2 MB/s \n",
      "\u001B[?25hCollecting nes-py>=8.1.4\n",
      "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
      "\u001B[K     |████████████████████████████████| 77 kB 7.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.21.6)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.64.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (1.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py>=8.1.4->gym-super-mario-bros) (0.16.0)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for nes-py: filename=nes_py-8.2.1-cp37-cp37m-linux_x86_64.whl size=439215 sha256=cea531b5af7f50445720198896fdaa5cbfb6b19aad408a989bfefc76dad589ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/96/0e/22a8c7dbdf412d8e988286f223b223baf0f4ad90c9e699c56d\n",
      "Successfully built nes-py\n",
      "Installing collected packages: nes-py, gym-super-mario-bros\n",
      "  Attempting uninstall: nes-py\n",
      "    Found existing installation: nes-py 0.2.6\n",
      "    Uninstalling nes-py-0.2.6:\n",
      "      Successfully uninstalled nes-py-0.2.6\n",
      "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nes-py==0.2.6\n",
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Importing the required modules**"
   ],
   "metadata": {
    "id": "-bN-d2FDZYRX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X2y9wwJSFDbJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import pickle\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Preprocessing the environment**\n",
    "We use Wrappers to preprocess the environment data before sending it to the agent."
   ],
   "metadata": {
    "id": "AoxMtZM_ZlwZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0FI9MvNgF8Wk"
   },
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  Displays every 4th frame\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, env=None, skip=4):\n",
    "      super(MaxAndSkipEnv, self).__init__(env)\n",
    "      self._obs_buffer = collections.deque(maxlen=2)\n",
    "      self._skip = skip\n",
    "\n",
    "  def step(self, action):\n",
    "    total_reward = 0.0\n",
    "    done = None\n",
    "    for _ in range(self._skip):\n",
    "      obs, reward, done, info = self.env.step(action)\n",
    "      self._obs_buffer.append(obs)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "        break\n",
    "    max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "    return max_frame, total_reward, done, info\n",
    "\n",
    "  def reset(self):\n",
    "    self._obs_buffer.clear()\n",
    "    obs = self.env.reset()\n",
    "    self._obs_buffer.append(obs)\n",
    "    return obs\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples image to 84x84\n",
    "    Greyscales image\n",
    "\n",
    "    Returns numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "def make_env(env):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The show_state function renders the required environment"
   ],
   "metadata": {
    "id": "D6xKu_FiaPRI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U40_I7lbNDke"
   },
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Initializes the environment**"
   ],
   "metadata": {
    "id": "lWnAPaf8jZ09"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoOb58dJNJIz",
    "outputId": "baf4e418-dd38-4f22-d25d-d0c987cf87e9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.54509807, 0.54509807, 0.54509807, ..., 0.54509807,\n",
       "         0.54509807, 0.54509807],\n",
       "        [0.54509807, 0.54509807, 0.54509807, ..., 0.54509807,\n",
       "         0.54509807, 0.54509807],\n",
       "        [0.54509807, 0.54509807, 0.54509807, ..., 0.54509807,\n",
       "         0.54509807, 0.54509807],\n",
       "        ...,\n",
       "        [0.5647059 , 0.48235294, 0.4509804 , ..., 0.34901962,\n",
       "         0.5764706 , 0.3254902 ],\n",
       "        [0.43529412, 0.3764706 , 0.33333334, ..., 0.42745098,\n",
       "         0.49411765, 0.3254902 ],\n",
       "        [0.63529414, 0.5294118 , 0.44705883, ..., 0.44313726,\n",
       "         0.48235294, 0.28627452]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = make_env(env)  # Wraps the environment so that frames are grayscale\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **DQN Architecture**\n",
    "Our DQN is a convolutional neural network with 3 convolutional layers and two linear layers. It input shape we will provide is 4x84x84 and number of actions is equal to 5. For image based regressions convolutional neural networks are ideal and hence it has been chosen."
   ],
   "metadata": {
    "id": "2VhqFLYijpUp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JFtTs-0RNYWX"
   },
   "outputs": [],
   "source": [
    "class Deep_Q_Learning(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Deep_Q_Learning, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        output = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    remember: In the remember method we just push an experience on to the buffer\n",
    "              so that we can use that for later.\n",
    "    \n",
    "    recall: It samples a batch of experience from the memory.\n",
    "\n",
    "    experience replay: We have two Q-networks target net and local net. We copy \n",
    "                       the local weights to the target weights, sample from our\n",
    "                       memory buffer and apply the DQN update equation , i.e\n",
    "                       Q*(st, at)←Q*(st, at) + α(rt+1 + γmaxaQθ(st+1, a) - Q*(st, at))\n",
    "                       to update our weights which will allow the agent to learn.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_mem, batch_size, gamma, lr,\n",
    "                 epsilon_greedy, epsilon_min, epsilon_decay, pretrained):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "       \n",
    "        self.local_net = Deep_Q_Learning(state_space, action_space).to(self.device)\n",
    "        self.target_net = Deep_Q_Learning(state_space, action_space).to(self.device)\n",
    "            \n",
    "        if self.pretrained:\n",
    "            self.local_net.load_state_dict(torch.load(\"dq1.pt\", map_location=torch.device(self.device)))\n",
    "            self.target_net.load_state_dict(torch.load(\"dq2.pt\", map_location=torch.device(self.device)))\n",
    "                    \n",
    "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "        self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "        self.step = 0\n",
    "\n",
    "        # Create memory\n",
    "        self.max_mem = max_mem\n",
    "        if self.pretrained:\n",
    "            #self.State_Memory = torch.load(\"State_Memory.pt\")\n",
    "            self.State_Memory = torch.zeros(max_mem, *self.state_space)\n",
    "            self.Action_Memory = torch.load(\"Action_Memory.pt\")\n",
    "            self.Reward_Memory = torch.load(\"Reward_Memory.pt\")\n",
    "            #self.State2_Memory = torch.load(\"State2_Memory.pt\")\n",
    "            self.State2_Memory = torch.zeros(max_mem, *self.state_space)\n",
    "            self.Done_Memory = torch.load(\"Done_Memory.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.State_Memory = torch.zeros(max_mem, *self.state_space)\n",
    "            self.Action_Memory = torch.zeros(max_mem, 1)\n",
    "            self.Reward_Memory = torch.zeros(max_mem, 1)\n",
    "            self.State2_Memory = torch.zeros(max_mem, *self.state_space)\n",
    "            self.Done_Memory = torch.zeros(max_mem, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "        \n",
    "        self.mem_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.epsilon_greedy = epsilon_greedy\n",
    "        self.exploration_rate = epsilon_greedy\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.State_Memory[self.ending_position] = state.float()\n",
    "        self.Action_Memory[self.ending_position] = action.float()\n",
    "        self.Reward_Memory[self.ending_position] = reward.float()\n",
    "        self.State2_Memory[self.ending_position] = state2.float()\n",
    "        self.Done_Memory[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_mem  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_mem)\n",
    "        \n",
    "    def recall(self):\n",
    "        # Randomly sample 'batch size' experiences\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.mem_size)\n",
    "        \n",
    "        STATE = self.State_Memory[idx]\n",
    "        ACTION = self.Action_Memory[idx]\n",
    "        REWARD = self.Reward_Memory[idx]\n",
    "        STATE2 = self.State2_Memory[idx]\n",
    "        DONE = self.Done_Memory[idx]\n",
    "        \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action\n",
    "        \n",
    "        self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "\n",
    "        # Local net is used for the policy\n",
    "        return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "\n",
    "\n",
    "    def copy_model(self):\n",
    "        # Copy local net weights into target net\n",
    "        \n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \n",
    "        if self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.mem_size > self.num_in_queue:\n",
    "            return\n",
    "\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "            # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "        target = REWARD + torch.mul((self.gamma * \n",
    "                                        self.target_net(STATE2).max(1).values.unsqueeze(1)), \n",
    "                                        1 - DONE)\n",
    "\n",
    "        current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.epsilon_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.epsilon_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Training the model**\n",
    "In this section we will proceed to train the model. We will set the values of the relevant hyperparameters and train the model for 10000 episodes. To train the use the run function with training_mode set to True and pretrained set to False. This will run a loop for 10000 episodes and save the data in the form of \".pt\" files. Once training is complete and we have the required files in our current working directory, in the run function we set training_mode to False and pretrained to True. This renders a small screen in which Mario will attempt to complete the game based on information saved in the files."
   ],
   "metadata": {
    "id": "cxNqYj0NtobP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1UHRpVQUUaIu"
   },
   "outputs": [],
   "source": [
    "def run(training_mode, pretrained):\n",
    "   \n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "    env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_mem=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     epsilon_greedy=1.0,\n",
    "                     epsilon_min=0.02,\n",
    "                     epsilon_decay=0.99,\n",
    "                     pretrained=pretrained)\n",
    "    \n",
    "    num_episodes = 10000\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor(np.array([state]))\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                #show_state(env, ep_num)\n",
    "                pass\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor(np.array([state_next]))\n",
    "            reward = torch.tensor(np.array([reward])).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                print(\"state\", state)\n",
    "                print(\"action\", action)\n",
    "                print(\"reward\", reward)\n",
    "                print(\"state_next\", state_next)\n",
    "                print(\"terminal\", terminal)\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1      \n",
    "    \n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "        \n",
    "        torch.save(agent.local_net.state_dict(), \"dq1.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), \"dq2.pt\")\n",
    "        torch.save(agent.State_Memory,  \"State_Memory.pt\")\n",
    "        torch.save(agent.Action_Memory, \"Action_Memory.pt\")\n",
    "        torch.save(agent.Reward_Memory, \"Reward_Memory.pt\")\n",
    "        torch.save(agent.State2_Memory, \"State2_Memory.pt\")\n",
    "        torch.save(agent.Done_Memory,   \"Done_Memory.pt\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    if num_episodes > 500:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(500)] + \n",
    "                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n",
    "        plt.show()\n",
    "#run(training_mode=True, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Testing a pretrained model**\n",
    "A model has already been trained over 10000 episodess for the convenience of the users of this notebook and the relevant files are saved in the Github repository. Training through all the 10000 episodes after training, have concluded that the model has a win rate of around 0.5%. Since it might not be possible for the user of this notebook to go through a large number of episodes, for their convenience we chosen the episode with maximum reward and ran it using the show_state function. To use this run the test_model function at the end after running all the cells below."
   ],
   "metadata": {
    "id": "3XAEboWn2BRE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def give_agent(training_mode, pretrained):\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "    env = make_env(env) \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_mem=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     epsilon_greedy=1.0,\n",
    "                     epsilon_min=0.02,\n",
    "                     epsilon_decay=0.99,\n",
    "                     pretrained=pretrained)\n",
    "    print(agent.Action_Memory.shape)\n",
    "    return env, agent"
   ],
   "metadata": {
    "id": "SagCLQM8yEjU"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "envi, mario_agent = give_agent(False, True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U_JSUI3yXly",
    "outputId": "8e4cb8b3-015b-4946-81a3-59f0e29109b0"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([30000, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "count = 0\n",
    "reward_counter = 0\n",
    "begin = 0\n",
    "max_reward = 0\n",
    "max_begin = 0\n",
    "max_end = 0\n",
    "num = 0\n",
    "for i in mario_agent.Done_Memory:\n",
    "  reward_counter += mario_agent.Reward_Memory[count].tolist()[0]\n",
    "  if i == torch.Tensor([1.0]):\n",
    "    num += 1\n",
    "    if reward_counter > max_reward:\n",
    "      max_reward = reward_counter\n",
    "      max_begin = begin\n",
    "      max_end = count\n",
    "    begin = count\n",
    "    reward_counter = 0\n",
    "  count += 1"
   ],
   "metadata": {
    "id": "saJmZnD94R-8"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "action_list = []\n",
    "action_counter = max_begin + 1\n",
    "reward_counter = 0\n",
    "while action_counter <= max_end:\n",
    "  this_action = int(mario_agent.Action_Memory[action_counter].tolist()[0])\n",
    "  reward_counter += mario_agent.Reward_Memory[action_counter].tolist()[0]\n",
    "  action_list.append(this_action)\n",
    "  action_counter += 1"
   ],
   "metadata": {
    "id": "Z1uKUM1s7YoZ"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model():\n",
    "    agent = mario_agent \n",
    "\n",
    "    env = gym.wrappers.Monitor(envi, \"./gym-results\", force=True)\n",
    "    env.reset()\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(np.array([state]))\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while steps <= max_end:\n",
    "        show_state(env)\n",
    "\n",
    "        action = action_list[steps]\n",
    "        steps += 1\n",
    "        \n",
    "        state_next, reward, terminal, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        state_next = torch.Tensor(np.array([state_next]))\n",
    "        reward = torch.tensor(np.array([reward])).unsqueeze(0)\n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)            \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "    print(\"Total reward after episode  is {}\".format(total_reward))  "
   ],
   "metadata": {
    "id": "uFnKAVOJwXot"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "OQoa3Y2SUwKm",
    "outputId": "a34abc0b-1a42-4514-9029-ad0fd3ed3808",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total reward after episode  is 3069.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#run(training_mode=False, pretrained=True)\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0Pgfnxp4U3AI"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VLG_Project.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}